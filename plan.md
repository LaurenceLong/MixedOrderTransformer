以下是基于你的 `MixedTokenizer` 设计，使用 `pythia` 模型进行预训练、SFT（监督微调）和 RLHF（人类反馈强化学习）的完整实验流程和代码实现。实验过程包括对比正序生成和逆序生成结果的准确性，验证你的设计思路。

---

## **实验流程**

实验分为以下三个阶段：

### **1. 预训练阶段**
- 使用 `pythia` 模型（如 `pythia-70m`）加载预训练权重，替换其 tokenizer 为你设计的 `MixedTokenizer`。
- 数据集：基于数学任务（如四则运算、乘法表等）构造训练数据，混入一定比例的逆序数据。
- 对比：
  - **无逆序数据**：仅使用正序生成数据进行训练。
  - **混合数据**：混入一定比例（如 25%）的逆序生成数据，观察模型的数学任务泛化能力。

---

### **2. 监督微调（SFT）阶段**
- 使用 `pythia` 模型在预训练基础上进行微调，专注于数学任务（如加减乘除）。
- 数据集：针对数学任务构造更复杂的训练集，并确保正序和逆序数据均有标注。
- 对比：
  - **正序微调**：仅使用正序数据。
  - **混合微调**：混入一定比例逆序数据，并通过 `<BOB>` 和 `<EOB>` 标签标记逆序区域。

---

### **3. RLHF 阶段**
- 通过奖励模型（Reward Model, RM）强化逆序生成的正确性。
- 数据集：使用生成模型产生样本，结合数学任务的准确性和生成顺序（逆序偏好）设计奖励函数。
- 对比：
  - **正序强化**：偏好正序生成的正确性。
  - **逆序强化**：偏好逆序生成的正确性。

---

## **代码实现**
略

---

### **5. 实验对比与评估**

通过以下方式对比不同阶段的模型性能：

1. **预训练与 SFT 对比**  
   测试加减乘除任务的准确性，验证混入逆序数据的效果。

2. **RLHF 强化效果**  
   测试逆序生成结果的正确率，观察 RLHF 是否成功强化了逆序生成。

3. **生成案例对比**  
   在相同输入下，对比正序和逆序生成的质量与准确性。

---

## **总结**

- 此代码基于你的 `MixedTokenizer` 替换了默认 tokenizer，并针对数学任务实现了预训练、SFT 和 RLHF。
- 通过对比实验，可以验证混合逆序生成和 RLHF 的实际效果。
- 如果需要更复杂的奖励函数或模型结构，可以进一步扩展此代码。